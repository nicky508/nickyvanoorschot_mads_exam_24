2024-06-25 12:07:22.772 | INFO     | __main__:main:26 - starting exam.py
2024-06-25 12:07:22.943 | INFO     | __main__:main:55 - the shape before : torch.Size([32, 192, 1])
2024-06-25 12:07:23.042 | INFO     | __main__:main:58 - the latent shape : torch.Size([32, 64])
2024-06-25 12:07:23.065 | INFO     | __main__:main:61 - the shape after: torch.Size([32, 192, 1])
2024-06-25 12:07:23.066 | INFO     | __main__:main:65 - Untrained loss: 16.434694290161133
2024-06-25 12:07:23.066 | INFO     | __main__:main:67 - starting training for 100 epochs
2024-06-25 12:07:44.678 | INFO     | __main__:main:26 - starting exam.py
2024-06-25 12:07:44.854 | INFO     | __main__:main:55 - the shape before : torch.Size([32, 192, 1])
2024-06-25 12:07:44.946 | INFO     | __main__:main:58 - the latent shape : torch.Size([32, 64])
2024-06-25 12:07:44.963 | INFO     | __main__:main:61 - the shape after: torch.Size([32, 192, 1])
2024-06-25 12:07:44.964 | INFO     | __main__:main:65 - Untrained loss: 15.013750076293945
2024-06-25 12:07:44.964 | INFO     | __main__:main:67 - starting training for 100 epochs
2024-06-25 12:08:01.375 | INFO     | __main__:main:26 - starting exam.py
2024-06-25 12:08:01.550 | INFO     | __main__:main:55 - the shape before : torch.Size([32, 192, 1])
2024-06-25 12:08:01.642 | INFO     | __main__:main:58 - the latent shape : torch.Size([32, 64])
2024-06-25 12:08:01.657 | INFO     | __main__:main:61 - the shape after: torch.Size([32, 192, 1])
2024-06-25 12:08:01.658 | INFO     | __main__:main:65 - Untrained loss: 12.190176010131836
2024-06-25 12:08:01.658 | INFO     | __main__:main:67 - starting training for 100 epochs
2024-06-25 12:08:44.708 | INFO     | __main__:main:26 - starting exam.py
2024-06-25 12:08:44.872 | INFO     | __main__:main:55 - the shape before : torch.Size([32, 192, 1])
2024-06-25 12:08:44.967 | INFO     | __main__:main:58 - the latent shape : torch.Size([32, 64])
2024-06-25 12:08:44.985 | INFO     | __main__:main:61 - the shape after: torch.Size([32, 192, 1])
2024-06-25 12:08:44.986 | INFO     | __main__:main:65 - Untrained loss: 16.553064346313477
2024-06-25 12:08:44.987 | INFO     | __main__:main:67 - starting training for 100 epochs
2024-06-25 12:11:52.565 | INFO     | __main__:main:26 - starting exam.py
2024-06-25 12:12:18.789 | INFO     | __main__:main:26 - starting exam.py
2024-06-25 12:12:18.970 | INFO     | __main__:main:55 - the shape before : torch.Size([32, 192, 1])
2024-06-25 12:12:19.056 | INFO     | __main__:main:58 - the latent shape : torch.Size([32, 64])
2024-06-25 12:12:19.074 | INFO     | __main__:main:61 - the shape after: torch.Size([32, 192, 1])
2024-06-25 12:12:19.075 | INFO     | __main__:main:65 - Untrained loss: 7.097777366638184
2024-06-25 12:12:19.075 | INFO     | __main__:main:67 - starting training for 100 epochs
2024-06-25 12:12:19.092 | INFO     | mltrainer.trainer:dir_add_timestamp:29 - Logging to logs/20240625-121219
2024-06-25 12:12:19.681 | INFO     | mltrainer.trainer:__init__:72 - Found earlystop_kwargs in settings.Set to None if you dont want earlystopping.
2024-06-25 12:30:43.923 | INFO     | __main__:main:26 - starting exam.py
2024-06-25 12:30:44.089 | INFO     | __main__:main:55 - the shape before : torch.Size([32, 192, 1])
2024-06-25 12:30:44.175 | INFO     | __main__:main:58 - the latent shape : torch.Size([32, 64])
2024-06-25 12:31:32.140 | INFO     | __main__:main:26 - starting exam.py
2024-06-25 12:31:32.309 | INFO     | __main__:main:55 - the shape before : torch.Size([32, 192, 1])
2024-06-25 12:31:32.400 | INFO     | __main__:main:58 - the latent shape : torch.Size([32, 64])
2024-06-25 12:37:47.826 | INFO     | __main__:main:26 - starting exam.py
2024-06-25 12:37:47.998 | INFO     | __main__:main:55 - the shape before : torch.Size([32, 192, 1])
2024-06-25 12:37:48.088 | INFO     | __main__:main:58 - the latent shape : torch.Size([32, 64])
2024-06-25 12:41:10.915 | INFO     | __main__:main:26 - starting exam.py
2024-06-25 12:41:11.075 | INFO     | __main__:main:55 - the shape before : torch.Size([32, 192, 1])
2024-06-25 12:41:11.163 | INFO     | __main__:main:58 - the latent shape : torch.Size([32, 64])
2024-06-25 12:41:26.805 | INFO     | __main__:main:26 - starting exam.py
2024-06-25 12:41:26.970 | INFO     | __main__:main:55 - the shape before : torch.Size([32, 192, 1])
2024-06-25 12:41:27.052 | INFO     | __main__:main:58 - the latent shape : torch.Size([32, 64])
2024-06-25 12:41:27.081 | INFO     | __main__:main:61 - the shape after: torch.Size([32, 192, 1])
2024-06-25 12:41:27.082 | INFO     | __main__:main:65 - Untrained loss: 8.266090393066406
2024-06-25 12:41:27.082 | INFO     | __main__:main:67 - starting training for 100 epochs
2024-06-25 12:41:27.101 | INFO     | mltrainer.trainer:dir_add_timestamp:29 - Logging to logs/20240625-124127
2024-06-25 12:41:27.711 | INFO     | mltrainer.trainer:__init__:72 - Found earlystop_kwargs in settings.Set to None if you dont want earlystopping.
2024-06-25 12:42:01.610 | INFO     | __main__:main:26 - starting exam.py
2024-06-25 12:42:01.819 | INFO     | __main__:main:55 - the shape before : torch.Size([32, 192, 1])
2024-06-25 12:42:01.905 | INFO     | __main__:main:58 - the latent shape : torch.Size([32, 64])
2024-06-25 12:42:01.932 | INFO     | __main__:main:61 - the shape after: torch.Size([32, 192, 1])
2024-06-25 12:42:01.933 | INFO     | __main__:main:65 - Untrained loss: 17.037322998046875
2024-06-25 12:42:01.933 | INFO     | __main__:main:67 - starting training for 100 epochs
2024-06-25 12:42:01.954 | INFO     | mltrainer.trainer:dir_add_timestamp:29 - Logging to logs/20240625-124201
2024-06-25 12:42:02.528 | INFO     | mltrainer.trainer:__init__:72 - Found earlystop_kwargs in settings.Set to None if you dont want earlystopping.
2024-06-25 12:44:07.219 | INFO     | __main__:main:26 - starting exam.py
2024-06-25 12:44:07.459 | INFO     | __main__:main:55 - the shape before : torch.Size([32, 192, 1])
2024-06-25 12:44:07.600 | INFO     | __main__:main:58 - the latent shape : torch.Size([32, 64])
2024-06-25 12:44:07.670 | INFO     | __main__:main:61 - the shape after: torch.Size([32, 192, 1])
2024-06-25 12:44:07.672 | INFO     | __main__:main:65 - Untrained loss: 8.938387870788574
2024-06-25 12:44:07.673 | INFO     | __main__:main:67 - starting training for 100 epochs
2024-06-25 12:44:07.713 | INFO     | mltrainer.trainer:dir_add_timestamp:29 - Logging to logs/20240625-124407
2024-06-25 12:44:08.539 | INFO     | mltrainer.trainer:__init__:72 - Found earlystop_kwargs in settings.Set to None if you dont want earlystopping.
